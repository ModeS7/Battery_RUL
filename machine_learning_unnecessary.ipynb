{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-16T11:39:40.936724Z",
     "start_time": "2025-01-16T11:39:40.931214Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:39:40.981471Z",
     "start_time": "2025-01-16T11:39:40.959056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load CSV data into a pandas DataFrame\n",
    "df = pd.read_csv('data/Battery_RUL_cleaned.csv')\n",
    "df = df[df.columns[1:]]  # Remove the first column\n",
    "\n",
    "# Last column is the target variable\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "1d8f90ff8634e989",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:39:41.020643Z",
     "start_time": "2025-01-16T11:39:41.005323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define models\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, h_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, h_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_size, h_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_size//2, h_size//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_size//4, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, h_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, h_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "class NN(nn.Module):\n",
    "    def __init__(self, h_size, d_rate):\n",
    "        super().__init__()\n",
    "        self.Flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, h_size),\n",
    "            nn.BatchNorm1d(h_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(d_rate),\n",
    "            nn.Linear(h_size, h_size//2),\n",
    "            nn.BatchNorm1d(h_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_size//2, h_size//4),\n",
    "            nn.BatchNorm1d(h_size//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_size//4, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Flatten(x)\n",
    "        logit = self.linear_relu_stack(x)\n",
    "        return logit\n",
    "\n",
    "class LSTMNN(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(7, hidden_size, num_layers, batch_first=True, dropout=dropout_rate) # nn.GRU for Gated Recurrent Unit\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Take the output at the last timestep\n",
    "        return out\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_filters, kernel_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(7, num_filters, kernel_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.fc = nn.Linear(num_filters, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Change shape to (batch, channels, features)\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model = SimpleNN(10).to(device)\n",
    "#model = NN(512, 0.2).to(device)\n",
    "#model = NeuralNetwork(512).to(device)\n",
    "model = LSTMNN(32, 1, 0).to(device)\n",
    "#model = CNN(16, 3).to(device)\n",
    "print(model)"
   ],
   "id": "6e7a1a15a9029c0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "LSTMNN(\n",
      "  (lstm): LSTM(7, 32, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:39:41.082033Z",
     "start_time": "2025-01-16T11:39:41.074311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ],
   "id": "ea8b87d719a1a69c",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:39:41.116752Z",
     "start_time": "2025-01-16T11:39:41.110817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(dataloader, model, loss1, loss2, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        X, y = batch\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = 0.7 * loss1(pred.squeeze(), y) + 0.3 * loss2(pred.squeeze(), y)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Log the loss\n",
    "        writer.add_scalar('Training Loss', loss.item(), epoch)\n",
    "        return loss\n"
   ],
   "id": "34739c3dbd34ef58",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:39:41.147696Z",
     "start_time": "2025-01-16T11:39:41.141745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(dataloader, model, loss1, loss2, epoch):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X).squeeze()\n",
    "            test_loss += loss1(pred, y).item() + loss2(pred, y).item()\n",
    "    test_loss /= num_batches\n",
    "\n",
    "    # Log the loss\n",
    "    writer.add_scalar('Test Loss', test_loss, epoch)\n",
    "    return test_loss\n"
   ],
   "id": "25961fea9b78cc4a",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:39:42.470316Z",
     "start_time": "2025-01-16T11:39:41.316990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter('runs/experiment_LSTM')\n",
    "\n",
    "loss1 = nn.MSELoss()\n",
    "loss2 = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.8)\n",
    "epochs = 1500\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = EarlyStopping(patience=50, min_delta=0.01)\n",
    "\n",
    "pbar = tqdm(range(epochs))\n",
    "for t in pbar:\n",
    "    train_loss = train(train_dataloader, model, loss1, loss2, optimizer, t)\n",
    "    test_loss = test(test_dataloader, model, loss1, loss2, t)\n",
    "    test_loss_MAE = test(test_dataloader, model, nn.L1Loss(), nn.L1Loss(), t)\n",
    "    test_loss_MSE = test(test_dataloader, model, nn.MSELoss(), nn.MSELoss(), t)\n",
    "    scheduler.step()\n",
    "    pbar.set_description(f\"Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Loss MAE: {test_loss_MAE:.4f}, Test Loss MSE: {test_loss_MSE:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    #early_stopping(test_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "print(\"Done!\")\n",
    "torch.save(model.state_dict(), 'models/model.pt')  # Save only the state_dict\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n",
    "\n",
    "model.eval()\n",
    "X, y = next(iter(test_dataloader))\n",
    "X, y = X.to(device), y.to(device)\n",
    "with torch.no_grad():\n",
    "    pred = model(X)\n",
    "    predicted, actual = pred[0], y[0]\n",
    "    print(f'Input: \"{X[0]}\", Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ],
   "id": "8cb3510bee6b1130",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 341633.4062, Test Loss: 833062.7398:   1%|          | 12/1500 [00:00<01:41, 14.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[59], line 16\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m pbar:\n\u001B[0;32m     15\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m train(train_dataloader, model, loss1, loss2, optimizer, t)\n\u001B[1;32m---> 16\u001B[0m     test_loss \u001B[38;5;241m=\u001B[39m test(test_dataloader, model, loss1, loss2, t)\n\u001B[0;32m     17\u001B[0m     scheduler\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     18\u001B[0m     pbar\u001B[38;5;241m.\u001B[39mset_description(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Test Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[58], line 9\u001B[0m, in \u001B[0;36mtest\u001B[1;34m(dataloader, model, loss1, loss2, epoch)\u001B[0m\n\u001B[0;32m      7\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m batch\n\u001B[0;32m      8\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mto(device), y\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m----> 9\u001B[0m         pred \u001B[38;5;241m=\u001B[39m model(X)\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[0;32m     10\u001B[0m         test_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss1(pred, y)\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m+\u001B[39m loss2(pred, y)\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     11\u001B[0m test_loss \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m num_batches\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[55], line 77\u001B[0m, in \u001B[0;36mLSTMNN.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     76\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 77\u001B[0m     out, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlstm(x)\n\u001B[0;32m     78\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(out[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :])  \u001B[38;5;66;03m# Take the output at the last timestep\u001B[39;00m\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\rnn.py:1123\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m   1120\u001B[0m         hx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hx, sorted_indices)\n\u001B[0;32m   1122\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1123\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\n\u001B[0;32m   1124\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   1125\u001B[0m         hx,\n\u001B[0;32m   1126\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights,\n\u001B[0;32m   1127\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias,\n\u001B[0;32m   1128\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers,\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout,\n\u001B[0;32m   1130\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining,\n\u001B[0;32m   1131\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional,\n\u001B[0;32m   1132\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first,\n\u001B[0;32m   1133\u001B[0m     )\n\u001B[0;32m   1134\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1135\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\n\u001B[0;32m   1136\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   1137\u001B[0m         batch_sizes,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1144\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional,\n\u001B[0;32m   1145\u001B[0m     )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 59
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
