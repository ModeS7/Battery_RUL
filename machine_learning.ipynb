{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T09:24:48.269431Z",
     "start_time": "2025-02-03T09:24:40.584202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, make_scorer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge, LogisticRegression, HuberRegressor, TheilSenRegressor, PoissonRegressor, TweedieRegressor, GammaRegressor, SGDRegressor, OrthogonalMatchingPursuit, PassiveAggressiveRegressor, RANSACRegressor, ElasticNetCV, OrthogonalMatchingPursuitCV, LarsCV, LassoCV, RidgeCV, ARDRegression, LassoLars\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor,HistGradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, VotingRegressor, RandomTreesEmbedding, IsolationForest\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "id": "e8c056c1d9f28e8d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T12:15:35.684668Z",
     "start_time": "2025-01-27T12:15:35.626985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load CSV data into a pandas DataFrame\n",
    "df = pd.read_csv('data/Battery_RUL_cleaned.csv')\n",
    "#df = pd.read_csv('data/Battery_RUL.csv')\n",
    "df = df[df.columns[1:]]  # Remove the first column\n",
    "\n",
    "# Last column is the target variable\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ],
   "id": "d1aeaf59b6a1f90e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Combine X_test and y_test into a single DataFrame\n",
    "test_data = pd.DataFrame(X_test, columns=df.columns[:-1])\n",
    "test_data['RUL'] = y_test\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "test_data.to_csv('data/test_data.csv', index=False)"
   ],
   "id": "368fdf10279a73a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T12:30:41.845374Z",
     "start_time": "2025-01-20T12:30:41.836617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_grid_search_results(results, param1, param2, score1, score2):\n",
    "    \"\"\"\n",
    "    Plots the grid search results with two y-axes for different hyperparameter values.\n",
    "\n",
    "    Parameters:\n",
    "    - results: DataFrame containing the GridSearchCV results.\n",
    "    - param1: The first hyperparameter to plot on the x-axis.\n",
    "    - param2: The second hyperparameter to differentiate the lines.\n",
    "    - score1: The first score to plot on the primary y-axis.\n",
    "    - score2: The second score to plot on the secondary y-axis.\n",
    "    \"\"\"\n",
    "    # Create a single plot with two y-axes\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot the first score on the primary y-axis\n",
    "    for value in results[param2].unique():\n",
    "        subset = results[results[param2] == value]\n",
    "        ax1.plot(subset[param1], -subset[score1], label=f'{score1} - {param2}: {value}', linestyle='-', marker='o')\n",
    "    ax1.set_xlabel(param1)\n",
    "    ax1.set_ylabel(score1)\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.set_title('Grid Search Results')\n",
    "\n",
    "    # Create a secondary y-axis for the second score\n",
    "    ax2 = ax1.twinx()\n",
    "    for value in results[param2].unique():\n",
    "        subset = results[results[param2] == value]\n",
    "        ax2.plot(subset[param1], -subset[score2], label=f'{score2} - {param2}: {value}', linestyle='--', marker='x')\n",
    "    ax2.set_ylabel(score2)\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "4e765e1fc194dd80",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T13:30:09.750732Z",
     "start_time": "2025-01-21T13:28:31.360111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define models to evaluate\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(n_jobs=-1),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"Support Vector Regressor\": SVR(),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=1000, n_jobs=-1),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=1000),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsRegressor(n_jobs=-1),\n",
    "    \"Multi-layer Perceptron\": MLPRegressor(),\n",
    "    \"Elasitc Net\": ElasticNet(),\n",
    "    \"Bayesian Ridge\": BayesianRidge(),\n",
    "    \"Extra Trees\": ExtraTreesRegressor(n_estimators=1000, n_jobs=-1),\n",
    "    \"AdaBoost\": AdaBoostRegressor(),\n",
    "    \"Bagging\": BaggingRegressor(n_estimators=1000, n_jobs=-1),\n",
    "    \"Hist Gradient Boosting\": HistGradientBoostingRegressor(),\n",
    "    \"CatBoost\": CatBoostRegressor(n_estimators=1000, verbose=0),\n",
    "    \"LightGBM\": LGBMRegressor(n_estimators=1000, n_jobs=-1),\n",
    "    \"XGBoost\": XGBRegressor(objective='reg:squarederror', n_estimators=1000, n_jobs=-1),\n",
    "    #\"Logistic Regression\": LogisticRegression(max_iter=100, n_jobs=-1),\n",
    "    \"Gaussian Process\": GaussianProcessRegressor(),\n",
    "    \"Huber Regressor\": HuberRegressor(),\n",
    "    \"Theil-Sen Regressor\": TheilSenRegressor(),\n",
    "    \"Poisson Regressor\": PoissonRegressor(),\n",
    "    \"Tweedie Regressor\": TweedieRegressor(),\n",
    "    \"Gamma Regressor\": GammaRegressor(),\n",
    "    #\"SGD Regressor\": SGDRegressor(),\n",
    "    \"Orthogonal Matching Pursuit\": OrthogonalMatchingPursuit(),\n",
    "    \"Passive Aggressive Regressor\": PassiveAggressiveRegressor(),\n",
    "    \"RANSAC Regressor\": RANSACRegressor(),\n",
    "    \"Elastic Net CV\": ElasticNetCV(),\n",
    "    \"Orthogonal Matching Pursuit CV\": OrthogonalMatchingPursuitCV(),\n",
    "    \"Lars CV\": LarsCV(),\n",
    "    \"Lasso CV\": LassoCV(),\n",
    "    \"Ridge CV\": RidgeCV(),\n",
    "    \"ARD Regression\": ARDRegression(),\n",
    "    \"Lasso Lars\": LassoLars(),\n",
    "    \"Isotonic Regression\": IsotonicRegression(),\n",
    "    \"Isolation Forest\": IsolationForest()\n",
    "}\n",
    "\n",
    "#models = {\n",
    "#    \"Random Trees Embedding\": RandomTreesEmbedding(n_estimators=1000, n_jobs=-1),\n",
    "#    \"Isolation Forest\": IsolationForest(),\n",
    "#    \"RBFSampler\": RBFSampler()}\n",
    "\n",
    "\n",
    "# Initialize a dataframe to store results\n",
    "results = []\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    if name == \"Isotonic Regression\":\n",
    "        X_train_model = X_train[:, 0].reshape(-1, 1)  # Select a single feature\n",
    "        X_test_model = X_test[:, 0].reshape(-1, 1)\n",
    "    else:\n",
    "        X_train_model = X_train\n",
    "        X_test_model = X_test\n",
    "    model.fit(X_train_model, y_train)\n",
    "    y_pred = model.predict(X_test_model)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results.append({\"Model\": name,\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2})\n",
    "    \n",
    "    \n",
    "\n",
    "# Convert results to a DataFrame and rank them\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"MAE\", ascending=True)\n",
    "#results_df.to_csv('data/ranking.csv', index=False)\n",
    "print(results_df)\n"
   ],
   "id": "2fe90323d8a9aaa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.183e+07, tolerance: 1.015e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.210e+07, tolerance: 1.015e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1769\n",
      "[LightGBM] [Info] Number of data points in the train set: 10799, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 583.079452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:285: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:285: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Model         MAE            MSE        RMSE  \\\n",
      "11                     Extra Trees    6.412304     162.933666   12.764547   \n",
      "13                         Bagging    7.944500     234.183018   15.303040   \n",
      "4                    Random Forest    7.962350     233.887156   15.293370   \n",
      "17                         XGBoost    8.668143     245.356706   15.663866   \n",
      "6                    Decision Tree    8.900000     527.425000   22.965735   \n",
      "16                        LightGBM    8.973900     239.627970   15.479922   \n",
      "15                        CatBoost   14.020744     410.249081   20.254606   \n",
      "5                Gradient Boosting   15.483649     505.315626   22.479227   \n",
      "14          Hist Gradient Boosting   16.714838     582.857668   24.142445   \n",
      "7              K-Nearest Neighbors   22.810074    1374.014252   37.067698   \n",
      "1                 Ridge Regression   34.076042    2103.815289   45.867366   \n",
      "26                RANSAC Regressor   34.081837    2114.950613   45.988592   \n",
      "31                        Ridge CV   34.110199    2082.317141   45.632413   \n",
      "32                  ARD Regression   34.138231    2080.600883   45.613604   \n",
      "29                         Lars CV   34.142392    2080.856114   45.616402   \n",
      "0                Linear Regression   34.142392    2080.856114   45.616402   \n",
      "34             Isotonic Regression   35.439309    2248.574201   47.419133   \n",
      "10                  Bayesian Ridge   35.803766    2331.903393   48.289786   \n",
      "28  Orthogonal Matching Pursuit CV   35.804113    2331.646044   48.287121   \n",
      "33                      Lasso Lars   35.805559    2331.591595   48.286557   \n",
      "22               Tweedie Regressor   35.814524    2334.255360   48.314132   \n",
      "9                      Elasitc Net   35.818380    2339.068964   48.363922   \n",
      "2                 Lasso Regression   35.818956    2339.026153   48.363480   \n",
      "27                  Elastic Net CV   36.626151    2424.090338   49.235052   \n",
      "30                        Lasso CV   36.654860    2427.652696   49.271216   \n",
      "24     Orthogonal Matching Pursuit   36.795948    2430.978535   49.304954   \n",
      "20             Theil-Sen Regressor   37.533760    2428.915503   49.284029   \n",
      "8           Multi-layer Perceptron   37.883602    2546.321406   50.461088   \n",
      "19                 Huber Regressor   37.897884    2644.003087   51.419871   \n",
      "25    Passive Aggressive Regressor   39.297151    2945.495553   54.272420   \n",
      "12                        AdaBoost   39.495635    2390.603642   48.893800   \n",
      "3         Support Vector Regressor   39.981383    3008.742449   54.852005   \n",
      "23                 Gamma Regressor   75.620336  529778.557610  727.858886   \n",
      "21               Poisson Regressor  263.032811   92577.438969  304.265409   \n",
      "18                Gaussian Process  567.522317  419274.500813  647.514093   \n",
      "35                Isolation Forest  578.476667  427275.383333  653.663050   \n",
      "\n",
      "          R2  \n",
      "11  0.998240  \n",
      "13  0.997470  \n",
      "4   0.997473  \n",
      "17  0.997349  \n",
      "6   0.994302  \n",
      "16  0.997411  \n",
      "15  0.995568  \n",
      "5   0.994541  \n",
      "14  0.993703  \n",
      "7   0.985156  \n",
      "1   0.977271  \n",
      "26  0.977151  \n",
      "31  0.977503  \n",
      "32  0.977522  \n",
      "29  0.977519  \n",
      "0   0.977519  \n",
      "34  0.975707  \n",
      "10  0.974807  \n",
      "28  0.974810  \n",
      "33  0.974810  \n",
      "22  0.974782  \n",
      "9   0.974730  \n",
      "2   0.974730  \n",
      "27  0.973811  \n",
      "30  0.973773  \n",
      "24  0.973737  \n",
      "20  0.973759  \n",
      "8   0.972490  \n",
      "19  0.971435  \n",
      "25  0.968178  \n",
      "12  0.974173  \n",
      "3   0.967495  \n",
      "23 -4.723538  \n",
      "21 -0.000173  \n",
      "18 -3.529691  \n",
      "35 -3.616130  \n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:05:40.719744Z",
     "start_time": "2025-01-20T00:05:40.684744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#model = LGBMRegressor(n_estimators=10000, n_jobs=-1)\n",
    "#model = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "#Mean Absolute Error: 7.9007\n",
    "#Mean Squared Error: 211.5364\n",
    "#model = GradientBoostingRegressor(n_estimators=1000, max_depth=6)\n",
    "#Mean Absolute Error: 9.1008\n",
    "#Mean Squared Error: 256.1412\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ],
   "id": "f029fa143187a462",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 35.4393\n",
      "Mean Squared Error: 2248.5742\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T23:39:26.356680Z",
     "start_time": "2025-01-18T23:38:53.440616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the hyperparameter grid\n",
    "#param_grid = {\n",
    "#    \"n_estimators\": [100, 200, 500],\n",
    "#    \"max_depth\": [10, 20, 50, None],\n",
    "#    \"min_samples_split\": [2, 5, 10],\n",
    "#    \"min_samples_leaf\": [1, 2, 4],\n",
    "#    \"max_features\": [None, \"sqrt\", \"log2\"]}\n",
    "\n",
    "# Define the scoring dictionary\n",
    "#scoring = {\n",
    "#    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "#    'MSE': make_scorer(mean_squared_error, greater_is_better=False)}\n",
    "\n",
    "# Initialize the GridSearchCV with n_jobs=1\n",
    "model = ExtraTreesRegressor(n_estimators=10000, n_jobs=-1)\n",
    "#model = GridSearchCV(ExtraTreesRegressor(), param_grid, cv=5, scoring=scoring, refit='MSE', n_jobs=1, verbose=3)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Extract the results and convert to a DataFrame\n",
    "#results = pd.DataFrame(model.cv_results_)\n",
    "#results.to_csv('data/ETR1.csv', index=False)\n",
    "\n",
    "# Get the best model\n",
    "#best_model = model.best_estimator_\n",
    "#print(best_model)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ],
   "id": "1234811bb559d5b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 6.3922\n",
      "Mean Squared Error: 161.5680\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the hyperparameter grid\n",
    "#param_grid = [\n",
    "#    {'C': [1e+4, 1e+5], 'kernel': ['rbf'], 'gamma': [1e-5, 1e-4]}]\n",
    "# Define the scoring dictionary\n",
    "#scoring = {\n",
    "#    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "#    'MSE': make_scorer(mean_squared_error, greater_is_better=False)}\n",
    "# Initialize and train the SVM model\n",
    "model = SVR(kernel='linear', C=0.0001)\n",
    "# Mean Absolute Error: 35.6885\n",
    "# Mean Squared Error: 2358.7762\n",
    "#model = GridSearchCV(SVR(), param_grid, cv=5, scoring=scoring, refit='MSE', n_jobs=-1, verbose=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Extract the results and convert to a DataFrame\n",
    "#results = pd.DataFrame(model.cv_results_)\n",
    "#results.to_csv('data/SVR2.csv', index=False)\n",
    "\n",
    "#plot_grid_search_results(results, 'param_C', 'param_gamma', 'mean_test_MSE', 'mean_test_MAE')\n",
    "\n",
    "# Get the best model\n",
    "#best_model = model.best_estimator_\n",
    "#print(best_model)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "# first run: 39.9814"
   ],
   "id": "2092498e7cfdfb6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:29:19.381089Z",
     "start_time": "2025-01-18T00:29:12.112078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the hyperparameter grid\n",
    "#param_grid = [\n",
    "#    {'max_depth': [None, 1, 2, 3, 4, 5, 6], 'n_estimators': [1000] }]\n",
    "# Define the scoring dictionary\n",
    "#scoring = {\n",
    "#    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "#    'MSE': make_scorer(mean_squared_error, greater_is_better=False)}\n",
    "# Initialize and train the Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=1000,  n_jobs=-1)\n",
    "#Mean Absolute Error: 7.9780\n",
    "#Mean Squared Error: 236.9407\n",
    "#model = GridSearchCV(RandomForestRegressor(), param_grid, cv=5, scoring=scoring, refit='MSE', n_jobs=-1, verbose=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Extract the results and convert to a DataFrame\n",
    "#results = pd.DataFrame(model.cv_results_)\n",
    "#results.to_csv('data/RFR1.csv', index=False)\n",
    "\n",
    "# Plot the grid search results\n",
    "#plot_grid_search_results(results, 'param_max_depth', 'param_n_estimators', 'mean_test_MSE', 'mean_test_MAE')\n",
    "\n",
    "# Get the best model\n",
    "#best_model = model.best_estimator_\n",
    "#print(best_model)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "# first run: 8.0703\n",
    "# best run: 7.9780"
   ],
   "id": "7ac2e03657bdb3e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 7.9351\n",
      "Mean Squared Error: 233.6358\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:17:39.457307Z",
     "start_time": "2025-01-18T00:17:31.612297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the hyperparameter grid\n",
    "#param_grid = [\n",
    "#    {'n_estimators': [1000, 10000, 100000, 1000000], 'learning_rate':[0.1], 'objective': ['reg:squarederror']}]\n",
    "# Define the scoring dictionary\n",
    "#scoring = {\n",
    "#    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "#    'MSE': make_scorer(mean_squared_error, greater_is_better=False)}\n",
    "# Initialize and train the XGBoost model\n",
    "model = XGBRegressor(objective='reg:squarederror', n_estimators=10000, learning_rate=0.1, n_jobs=-1)\n",
    "#Mean Absolute Error: 7.9632\n",
    "#Mean Squared Error: 210.3707\n",
    "#model = GridSearchCV(xgb.XGBRegressor(), param_grid, cv=5, scoring=scoring, refit='MSE', n_jobs=-1, verbose=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Extract the results and convert to a DataFrame\n",
    "#results = pd.DataFrame(model.cv_results_)\n",
    "#results.to_csv('data/XGB2.csv', index=False)\n",
    "\n",
    "# Plot the grid search results\n",
    "#plot_grid_search_results(results, 'param_n_estimators', 'param_learning_rate','mean_test_MSE', 'mean_test_MAE')\n",
    "\n",
    "# Get the best model\n",
    "#best_model = model.best_estimator_\n",
    "#print(best_model)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "# first run: 15.3975\n",
    "# best run: 7.9632"
   ],
   "id": "60f9f9bd5531afce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 7.9632\n",
      "Mean Squared Error: 210.3707\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T14:26:15.987991Z",
     "start_time": "2025-01-17T14:22:59.527698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# first run: 39.9814"
   ],
   "id": "4d3238d066f679f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 47.2215\n",
      "Mean Squared Error: 3565.5578\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:44:07.692138Z",
     "start_time": "2025-01-18T00:39:55.658762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train the CatBoost Regressor\n",
    "model = CatBoostRegressor(iterations=100000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "# best run:\n",
    "# Mean Absolute Error: 7.5714\n",
    "# Mean Squared Error: 186.3288"
   ],
   "id": "3873120eb6ba53b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 7.5714\n",
      "Mean Squared Error: 186.3288\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:52:43.193449Z",
     "start_time": "2025-01-18T00:52:25.794424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train the Gaussian Process Regressor\n",
    "model = GaussianProcessRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n"
   ],
   "id": "50bc2f0dc3bbd2b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 567.5223\n",
      "Mean Squared Error: 419274.5008\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T14:49:11.513130Z",
     "start_time": "2025-01-20T14:49:11.477181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to save real and predicted values to a CSV file\n",
    "def save_predictions_to_csv(model, X_test, y_test, filename):\n",
    "    y_pred = model.predict(X_test)\n",
    "    results_df = pd.DataFrame({'Real': y_test, 'Predicted': y_pred})\n",
    "    results_df.to_csv(filename, index=False)\n",
    "\n",
    "# Extra Trees Regressor\n",
    "model_et = ExtraTreesRegressor(n_estimators=10000, n_jobs=-1)\n",
    "model_et.fit(X_train, y_train)\n",
    "save_predictions_to_csv(model_et, X_test, y_test, 'data/predictions/Extra_Trees_Regression.csv')\n",
    "\n",
    "# Linear Regression\n",
    "model_lr = LinearRegression(n_jobs=-1)\n",
    "model_lr.fit(X_train, y_train)\n",
    "save_predictions_to_csv(model_lr, X_test, y_test, 'data/predictions/Linear_Regression.csv')\n",
    "\n",
    "# Gaussian Process Regressor\n",
    "model_gp = GaussianProcessRegressor(n_jobs=-1)\n",
    "model_gp.fit(X_train, y_train)\n",
    "save_predictions_to_csv(model_gp, X_test, y_test, 'data/predictions/Gaussian_Process_Regression.csv')\n",
    "\n",
    "# Poisson Regressor\n",
    "model_pr = PoissonRegressor()\n",
    "model_pr.fit(X_train, y_train)\n",
    "save_predictions_to_csv(model_pr, X_test, y_test, 'data/predictions/Poisson_Regression.csv')\n",
    "\n",
    "# Passive Aggressive Regressor\n",
    "model_par = PassiveAggressiveRegressor()\n",
    "model_par.fit(X_train, y_train)\n",
    "save_predictions_to_csv(model_par, X_test, y_test, 'data/predictions/Passive_Aggressive_Regression.csv')\n"
   ],
   "id": "361c2f5fed98ef3c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T12:27:48.201938Z",
     "start_time": "2025-01-27T12:27:47.367084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Example: Train a RandomForestRegressor\n",
    "model = ExtraTreesRegressor(n_estimators=100, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model to a file\n",
    "joblib.dump(model, 'models/ETR.pkl')\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ],
   "id": "24dd36fbd381c7f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 6.5888\n",
      "Mean Squared Error: 169.3640\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:11:58.748111Z",
     "start_time": "2025-01-21T15:08:44.992436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the trained model from a file\n",
    "model = joblib.load('models/ETR.pkl')\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ],
   "id": "45a834c743eb5276",
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.29 MiB for an array with shape (21127,) and data type {'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity', 'n_node_samples', 'weighted_n_node_samples', 'missing_go_to_left'], 'formats': ['<i8', '<i8', '<i8', '<f8', '<f8', '<i8', '<f8', 'u1'], 'offsets': [0, 8, 16, 24, 32, 40, 48, 56], 'itemsize': 64}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Load the trained model from a file\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m joblib\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels/ETR.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m      4\u001B[0m mae \u001B[38;5;241m=\u001B[39m mean_absolute_error(y_test, y_pred)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:658\u001B[0m, in \u001B[0;36mload\u001B[1;34m(filename, mmap_mode)\u001B[0m\n\u001B[0;32m    652\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fobj, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    653\u001B[0m                 \u001B[38;5;66;03m# if the returned file object is a string, this means we\u001B[39;00m\n\u001B[0;32m    654\u001B[0m                 \u001B[38;5;66;03m# try to load a pickle file generated with an version of\u001B[39;00m\n\u001B[0;32m    655\u001B[0m                 \u001B[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001B[39;00m\n\u001B[0;32m    656\u001B[0m                 \u001B[38;5;28;01mreturn\u001B[39;00m load_compatibility(fobj)\n\u001B[1;32m--> 658\u001B[0m             obj \u001B[38;5;241m=\u001B[39m _unpickle(fobj, filename, mmap_mode)\n\u001B[0;32m    659\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:577\u001B[0m, in \u001B[0;36m_unpickle\u001B[1;34m(fobj, filename, mmap_mode)\u001B[0m\n\u001B[0;32m    575\u001B[0m obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 577\u001B[0m     obj \u001B[38;5;241m=\u001B[39m unpickler\u001B[38;5;241m.\u001B[39mload()\n\u001B[0;32m    578\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m unpickler\u001B[38;5;241m.\u001B[39mcompat_mode:\n\u001B[0;32m    579\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe file \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m has been generated with a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    580\u001B[0m                       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjoblib version less than 0.10. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    581\u001B[0m                       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease regenerate this pickle file.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    582\u001B[0m                       \u001B[38;5;241m%\u001B[39m filename,\n\u001B[0;32m    583\u001B[0m                       \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\pickle.py:1205\u001B[0m, in \u001B[0;36m_Unpickler.load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1203\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEOFError\u001B[39;00m\n\u001B[0;32m   1204\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, bytes_types)\n\u001B[1;32m-> 1205\u001B[0m         dispatch[key[\u001B[38;5;241m0\u001B[39m]](\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m   1206\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _Stop \u001B[38;5;28;01mas\u001B[39;00m stopinst:\n\u001B[0;32m   1207\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m stopinst\u001B[38;5;241m.\u001B[39mvalue\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:415\u001B[0m, in \u001B[0;36mNumpyUnpickler.load_build\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    413\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(array_wrapper, NDArrayWrapper):\n\u001B[0;32m    414\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompat_mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 415\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstack\u001B[38;5;241m.\u001B[39mappend(array_wrapper\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mself\u001B[39m))\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:252\u001B[0m, in \u001B[0;36mNumpyArrayWrapper.read\u001B[1;34m(self, unpickler)\u001B[0m\n\u001B[0;32m    250\u001B[0m     array \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread_mmap(unpickler)\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 252\u001B[0m     array \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread_array(unpickler)\n\u001B[0;32m    254\u001B[0m \u001B[38;5;66;03m# Manage array subclass case\u001B[39;00m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mhasattr\u001B[39m(array, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__array_prepare__\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m    256\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubclass \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (unpickler\u001B[38;5;241m.\u001B[39mnp\u001B[38;5;241m.\u001B[39mndarray,\n\u001B[0;32m    257\u001B[0m                           unpickler\u001B[38;5;241m.\u001B[39mnp\u001B[38;5;241m.\u001B[39mmemmap)):\n\u001B[0;32m    258\u001B[0m     \u001B[38;5;66;03m# We need to reconstruct another subclass\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:173\u001B[0m, in \u001B[0;36mNumpyArrayWrapper.read_array\u001B[1;34m(self, unpickler)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;66;03m# This is not a real file. We have to read it the\u001B[39;00m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;66;03m# memory-intensive way.\u001B[39;00m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;66;03m# crc32 module fails on reads greater than 2 ** 32 bytes,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;66;03m# of the read. In non-chunked case count < max_read_count, so\u001B[39;00m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;66;03m# only one read is performed.\u001B[39;00m\n\u001B[0;32m    170\u001B[0m max_read_count \u001B[38;5;241m=\u001B[39m BUFFER_SIZE \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;28mmin\u001B[39m(BUFFER_SIZE,\n\u001B[0;32m    171\u001B[0m                                     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mitemsize)\n\u001B[1;32m--> 173\u001B[0m array \u001B[38;5;241m=\u001B[39m unpickler\u001B[38;5;241m.\u001B[39mnp\u001B[38;5;241m.\u001B[39mempty(count, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, count, max_read_count):\n\u001B[0;32m    175\u001B[0m     read_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(max_read_count, count \u001B[38;5;241m-\u001B[39m i)\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 1.29 MiB for an array with shape (21127,) and data type {'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity', 'n_node_samples', 'weighted_n_node_samples', 'missing_go_to_left'], 'formats': ['<i8', '<i8', '<i8', '<f8', '<f8', '<i8', '<f8', 'u1'], 'offsets': [0, 8, 16, 24, 32, 40, 48, 56], 'itemsize': 64}"
     ]
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
