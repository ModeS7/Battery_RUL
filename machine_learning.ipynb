{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T11:36:12.050911Z",
     "start_time": "2025-01-15T11:36:11.156848Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:54:05.393163Z",
     "start_time": "2025-01-14T20:54:05.360163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load CSV data into a pandas DataFrame\n",
    "df = pd.read_csv('data/Battery_RUL_cleaned.csv')\n",
    "df = df[df.columns[1:]]  # Remove the first column\n",
    "\n",
    "# Last column is the target variable\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "423dafd12ba1dc4a",
   "outputs": [],
   "execution_count": 325
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:54:05.455163Z",
     "start_time": "2025-01-14T20:54:05.441165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define models\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "class NN(nn.Module):\n",
    "    def __init__(self, h_size, d_rate):\n",
    "        super().__init__()\n",
    "        self.Flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, h_size),\n",
    "            nn.BatchNorm1d(h_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(d_rate),\n",
    "            nn.Linear(h_size, h_size//2),\n",
    "            nn.BatchNorm1d(h_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_size//2, h_size//4),\n",
    "            nn.BatchNorm1d(h_size//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_size//4, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Flatten(x)\n",
    "        logit = self.linear_relu_stack(x)\n",
    "        return logit\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "model = NN(512, 0.2).to(device)\n",
    "#model = NeuralNetwork().to(device)\n",
    "print(model)"
   ],
   "id": "a4b09eb20b1d9a70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NN(\n",
      "  (Flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 326
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:54:05.501162Z",
     "start_time": "2025-01-14T20:54:05.487164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ],
   "id": "682c8b476ba48405",
   "outputs": [],
   "execution_count": 327
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:54:05.562164Z",
     "start_time": "2025-01-14T20:54:05.548163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(dataloader, model, loss1, loss2, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        X, y = batch\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = 0.7 * loss1(pred.squeeze(), y) + 0.3 * loss2(pred.squeeze(), y)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Log the loss\n",
    "        writer.add_scalar('Training Loss', loss.item(), epoch)\n",
    "        return loss\n"
   ],
   "id": "4a8a22092ba25046",
   "outputs": [],
   "execution_count": 328
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:54:05.639164Z",
     "start_time": "2025-01-14T20:54:05.627164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(dataloader, model, loss1, loss2, epoch):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X).squeeze()\n",
    "            test_loss += loss1(pred, y).item() + loss2(pred, y).item()\n",
    "    test_loss /= num_batches\n",
    "\n",
    "    # Log the loss\n",
    "    writer.add_scalar('Test Loss', test_loss, epoch)\n",
    "    return test_loss\n"
   ],
   "id": "c5c8e0c44ee51852",
   "outputs": [],
   "execution_count": 329
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:55:39.272418Z",
     "start_time": "2025-01-14T20:54:05.688163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter('runs/experiment_2')\n",
    "\n",
    "loss1 = nn.L1Loss()\n",
    "loss2 = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.8)\n",
    "epochs = 1500\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = EarlyStopping(patience=50, min_delta=0.01)\n",
    "\n",
    "pbar = tqdm(range(epochs))\n",
    "for t in pbar:\n",
    "    train_loss = train(train_dataloader, model, loss1, loss2, optimizer, t)\n",
    "    test_loss = test(test_dataloader, model, loss1, loss2, t)\n",
    "    scheduler.step()\n",
    "    pbar.set_description(f\"Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    #early_stopping(test_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "print(\"Done!\")\n",
    "torch.save(model.state_dict(), 'model.pt')  # Save only the state_dict\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n",
    "\n",
    "model.eval()\n",
    "X, y = next(iter(test_dataloader))\n",
    "X, y = X.to(device), y.to(device)\n",
    "with torch.no_grad():\n",
    "    pred = model(X)\n",
    "    predicted, actual = pred[0], y[0]\n",
    "    print(f'Input: \"{X[0]}\", Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ],
   "id": "736f904d827bb8d2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 53.3328, Test Loss: 95.0207: 100%|██████████| 1500/1500 [01:33<00:00, 16.05it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Input: \"tensor([8.9362e+02, 2.3100e+02, 3.8010e+00, 3.7060e+00, 1.1923e+03, 1.7723e+03,\n",
      "        7.2443e+03], device='cuda:0')\", Predicted: \"tensor([48.1025], device='cuda:0')\", Actual: \"67.0\"\n"
     ]
    }
   ],
   "execution_count": 330
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:36:18.906671Z",
     "start_time": "2025-01-15T11:36:17.538422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary modules\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load CSV data into a pandas DataFrame\n",
    "df = pd.read_csv('data/Battery_RUL_cleaned.csv')\n",
    "df = df[df.columns[1:]]  # Remove the first column\n",
    "\n",
    "# Last column is the target variable\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ],
   "id": "e8c056c1d9f28e8d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:36:32.609167Z",
     "start_time": "2025-01-15T11:36:21.283884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train the SVM model\n",
    "model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")"
   ],
   "id": "2092498e7cfdfb6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 39.9814\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:36:43.809379Z",
     "start_time": "2025-01-15T11:36:36.748372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train the Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")"
   ],
   "id": "7ac2e03657bdb3e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 8.0703\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T21:10:40.509700Z",
     "start_time": "2025-01-14T21:10:40.386699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train the XGBoost model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")"
   ],
   "id": "60f9f9bd5531afce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 15.3975\n"
     ]
    }
   ],
   "execution_count": 340
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T21:30:11.152333Z",
     "start_time": "2025-01-14T21:26:34.338064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")"
   ],
   "id": "4d3238d066f679f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0085\n",
      "Mean Absolute Error: 47.2215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "execution_count": 345
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
