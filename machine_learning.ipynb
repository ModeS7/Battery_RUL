{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T23:23:47.849855Z",
     "start_time": "2025-01-18T23:23:47.835853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary modules\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, make_scorer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor,HistGradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n"
   ],
   "id": "e8c056c1d9f28e8d",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T22:33:09.077295Z",
     "start_time": "2025-01-18T22:33:09.043295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load CSV data into a pandas DataFrame\n",
    "df = pd.read_csv('data/Battery_RUL_cleaned.csv')\n",
    "#df = pd.read_csv('data/Battery_RUL.csv')\n",
    "df = df[df.columns[1:]]  # Remove the first column\n",
    "\n",
    "# Last column is the target variable\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "d1aeaf59b6a1f90e",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T22:25:53.095139Z",
     "start_time": "2025-01-18T22:25:53.085140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_grid_search_results(results, param1, param2, score1, score2):\n",
    "    \"\"\"\n",
    "    Plots the grid search results with two y-axes for different hyperparameter values.\n",
    "\n",
    "    Parameters:\n",
    "    - results: DataFrame containing the GridSearchCV results.\n",
    "    - param1: The first hyperparameter to plot on the x-axis.\n",
    "    - param2: The second hyperparameter to differentiate the lines.\n",
    "    - score1: The first score to plot on the primary y-axis.\n",
    "    - score2: The second score to plot on the secondary y-axis.\n",
    "    \"\"\"\n",
    "    # Create a single plot with two y-axes\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot the first score on the primary y-axis\n",
    "    for value in results[param2].unique():\n",
    "        subset = results[results[param2] == value]\n",
    "        ax1.plot(subset[param1], -subset[score1], label=f'{score1} - {param2}: {value}', linestyle='-', marker='o')\n",
    "    ax1.set_xlabel(param1)\n",
    "    ax1.set_ylabel(score1)\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.set_title('Grid Search Results')\n",
    "\n",
    "    # Create a secondary y-axis for the second score\n",
    "    ax2 = ax1.twinx()\n",
    "    for value in results[param2].unique():\n",
    "        subset = results[results[param2] == value]\n",
    "        ax2.plot(subset[param1], -subset[score2], label=f'{score2} - {param2}: {value}', linestyle='--', marker='x')\n",
    "    ax2.set_ylabel(score2)\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "4e765e1fc194dd80",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T23:36:01.001339Z",
     "start_time": "2025-01-18T23:35:22.170038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define models to evaluate\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"Support Vector Regressor\": SVR(),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsRegressor(),\n",
    "    \"Multi-layer Perceptron\": MLPRegressor(),\n",
    "    \"Elasitc Net\": ElasticNet(),\n",
    "    \"Bayesian Ridge\": BayesianRidge(),\n",
    "    \"Extra Trees\": ExtraTreesRegressor(n_estimators=1000),\n",
    "    \"AdaBoost\": AdaBoostRegressor(),\n",
    "    \"Bagging\": BaggingRegressor(),\n",
    "    \"Hist Gradient Boosting\": HistGradientBoostingRegressor(),\n",
    "    \"CatBoost\": CatBoostRegressor(verbose=0),\n",
    "    \"LightGBM\": LGBMRegressor(),\n",
    "    \"XGBoost\": XGBRegressor(objective='reg:squarederror')\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize a dataframe to store results\n",
    "results = []\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results.append({\"Model\": name,\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R²\": r2})\n",
    "\n",
    "# Convert results to a DataFrame and rank them\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"MAE\", ascending=True)\n",
    "print(results_df)\n"
   ],
   "id": "2fe90323d8a9aaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1769\n",
      "[LightGBM] [Info] Number of data points in the train set: 10799, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 583.079452\n",
      "                       Model        MAE          MSE       RMSE        R²\n",
      "11               Extra Trees   6.409673   161.862840  12.722533  0.998251\n",
      "4              Random Forest   8.037400   237.464521  15.409884  0.997435\n",
      "13                   Bagging   8.661406   291.514112  17.073784  0.996851\n",
      "6              Decision Tree   8.911481   564.532963  23.759902  0.993901\n",
      "17                   XGBoost  11.417299   323.778256  17.993839  0.996502\n",
      "15                  CatBoost  14.020744   410.249081  20.254606  0.995568\n",
      "16                  LightGBM  16.361694   550.314488  23.458783  0.994055\n",
      "14    Hist Gradient Boosting  16.452931   560.758489  23.680340  0.993942\n",
      "7        K-Nearest Neighbors  22.810074  1374.014252  37.067698  0.985156\n",
      "5          Gradient Boosting  26.687978  1326.691757  36.423780  0.985667\n",
      "1           Ridge Regression  34.076042  2103.815289  45.867366  0.977271\n",
      "0          Linear Regression  34.142392  2080.856114  45.616402  0.977519\n",
      "10            Bayesian Ridge  35.803766  2331.903393  48.289786  0.974807\n",
      "9                Elasitc Net  35.818380  2339.068964  48.363922  0.974730\n",
      "2           Lasso Regression  35.818956  2339.026153  48.363480  0.974730\n",
      "8     Multi-layer Perceptron  38.313139  2563.840780  50.634383  0.972301\n",
      "12                  AdaBoost  38.845203  2315.020572  48.114661  0.974989\n",
      "3   Support Vector Regressor  39.981383  3008.742449  54.852005  0.967495\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T23:39:26.356680Z",
     "start_time": "2025-01-18T23:38:53.440616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the hyperparameter grid\n",
    "#param_grid = {\n",
    "#    \"n_estimators\": [100, 200, 500],\n",
    "#    \"max_depth\": [10, 20, 50, None],\n",
    "#    \"min_samples_split\": [2, 5, 10],\n",
    "#    \"min_samples_leaf\": [1, 2, 4],\n",
    "#    \"max_features\": [None, \"sqrt\", \"log2\"]}\n",
    "\n",
    "# Define the scoring dictionary\n",
    "#scoring = {\n",
    "#    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "#    'MSE': make_scorer(mean_squared_error, greater_is_better=False)}\n",
    "\n",
    "# Initialize the GridSearchCV with n_jobs=1\n",
    "model = ExtraTreesRegressor(n_estimators=10000, n_jobs=-1)\n",
    "#model = GridSearchCV(ExtraTreesRegressor(), param_grid, cv=5, scoring=scoring, refit='MSE', n_jobs=1, verbose=3)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Extract the results and convert to a DataFrame\n",
    "#results = pd.DataFrame(model.cv_results_)\n",
    "#results.to_csv('data/ETR1.csv', index=False)\n",
    "\n",
    "# Get the best model\n",
    "#best_model = model.best_estimator_\n",
    "#print(best_model)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ],
   "id": "1234811bb559d5b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 6.3922\n",
      "Mean Squared Error: 161.5680\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the hyperparameter grid\n",
    "#param_grid = [\n",
    "#    {'C': [1e+4, 1e+5], 'kernel': ['rbf'], 'gamma': [1e-5, 1e-4]}]\n",
    "# Define the scoring dictionary\n",
    "#scoring = {\n",
    "#    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "#    'MSE': make_scorer(mean_squared_error, greater_is_better=False)}\n",
    "# Initialize and train the SVM model\n",
    "model = SVR(kernel='linear', C=0.0001)\n",
    "# Mean Absolute Error: 35.6885\n",
    "# Mean Squared Error: 2358.7762\n",
    "#model = GridSearchCV(SVR(), param_grid, cv=5, scoring=scoring, refit='MSE', n_jobs=-1, verbose=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Extract the results and convert to a DataFrame\n",
    "#results = pd.DataFrame(model.cv_results_)\n",
    "#results.to_csv('data/SVR2.csv', index=False)\n",
    "\n",
    "#plot_grid_search_results(results, 'param_C', 'param_gamma', 'mean_test_MSE', 'mean_test_MAE')\n",
    "\n",
    "# Get the best model\n",
    "#best_model = model.best_estimator_\n",
    "#print(best_model)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "# first run: 39.9814"
   ],
   "id": "2092498e7cfdfb6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:29:19.381089Z",
     "start_time": "2025-01-18T00:29:12.112078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the hyperparameter grid\n",
    "#param_grid = [\n",
    "#    {'max_depth': [None, 1, 2, 3, 4, 5, 6], 'n_estimators': [1000] }]\n",
    "# Define the scoring dictionary\n",
    "#scoring = {\n",
    "#    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "#    'MSE': make_scorer(mean_squared_error, greater_is_better=False)}\n",
    "# Initialize and train the Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=1000,  n_jobs=-1)\n",
    "#Mean Absolute Error: 7.9780\n",
    "#Mean Squared Error: 236.9407\n",
    "#model = GridSearchCV(RandomForestRegressor(), param_grid, cv=5, scoring=scoring, refit='MSE', n_jobs=-1, verbose=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Extract the results and convert to a DataFrame\n",
    "#results = pd.DataFrame(model.cv_results_)\n",
    "#results.to_csv('data/RFR1.csv', index=False)\n",
    "\n",
    "# Plot the grid search results\n",
    "#plot_grid_search_results(results, 'param_max_depth', 'param_n_estimators', 'mean_test_MSE', 'mean_test_MAE')\n",
    "\n",
    "# Get the best model\n",
    "#best_model = model.best_estimator_\n",
    "#print(best_model)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "# first run: 8.0703\n",
    "# best run: 7.9780"
   ],
   "id": "7ac2e03657bdb3e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 7.9351\n",
      "Mean Squared Error: 233.6358\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:17:39.457307Z",
     "start_time": "2025-01-18T00:17:31.612297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the hyperparameter grid\n",
    "#param_grid = [\n",
    "#    {'n_estimators': [1000, 10000, 100000, 1000000], 'learning_rate':[0.1], 'objective': ['reg:squarederror']}]\n",
    "# Define the scoring dictionary\n",
    "#scoring = {\n",
    "#    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "#    'MSE': make_scorer(mean_squared_error, greater_is_better=False)}\n",
    "# Initialize and train the XGBoost model\n",
    "model = XGBRegressor(objective='reg:squarederror', n_estimators=10000, learning_rate=0.1, n_jobs=-1)\n",
    "#Mean Absolute Error: 7.9632\n",
    "#Mean Squared Error: 210.3707\n",
    "#model = GridSearchCV(xgb.XGBRegressor(), param_grid, cv=5, scoring=scoring, refit='MSE', n_jobs=-1, verbose=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Extract the results and convert to a DataFrame\n",
    "#results = pd.DataFrame(model.cv_results_)\n",
    "#results.to_csv('data/XGB2.csv', index=False)\n",
    "\n",
    "# Plot the grid search results\n",
    "#plot_grid_search_results(results, 'param_n_estimators', 'param_learning_rate','mean_test_MSE', 'mean_test_MAE')\n",
    "\n",
    "# Get the best model\n",
    "#best_model = model.best_estimator_\n",
    "#print(best_model)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "# first run: 15.3975\n",
    "# best run: 7.9632"
   ],
   "id": "60f9f9bd5531afce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 7.9632\n",
      "Mean Squared Error: 210.3707\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T14:26:15.987991Z",
     "start_time": "2025-01-17T14:22:59.527698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# first run: 39.9814"
   ],
   "id": "4d3238d066f679f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 47.2215\n",
      "Mean Squared Error: 3565.5578\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:44:07.692138Z",
     "start_time": "2025-01-18T00:39:55.658762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train the CatBoost Regressor\n",
    "model = CatBoostRegressor(iterations=100000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "# best run:\n",
    "# Mean Absolute Error: 7.5714\n",
    "# Mean Squared Error: 186.3288"
   ],
   "id": "3873120eb6ba53b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 7.5714\n",
      "Mean Squared Error: 186.3288\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:52:43.193449Z",
     "start_time": "2025-01-18T00:52:25.794424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train the Gaussian Process Regressor\n",
    "model = GaussianProcessRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n"
   ],
   "id": "50bc2f0dc3bbd2b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 567.5223\n",
      "Mean Squared Error: 419274.5008\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T00:23:31.934392Z",
     "start_time": "2025-01-19T00:13:55.746184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to save real and predicted values to a CSV file\n",
    "def save_predictions_to_csv(model, X_test, y_test, filename):\n",
    "    y_pred = model.predict(X_test)\n",
    "    results_df = pd.DataFrame({'Real': y_test, 'Predicted': y_pred})\n",
    "    results_df.to_csv(filename, index=False)\n",
    "\n",
    "# Support Vector Regressor\n",
    "model_svr = SVR(kernel='linear', C=0.0001)\n",
    "model_svr.fit(X_train, y_train)\n",
    "save_predictions_to_csv(model_svr, X_test, y_test, 'data/predictions/Support_Vector_Regression.csv')\n",
    "\n",
    "# Random Forest Regressor\n",
    "model_rf = RandomForestRegressor(n_estimators=1000, n_jobs=-1)\n",
    "model_rf.fit(X_train, y_train)\n",
    "save_predictions_to_csv(model_rf, X_test, y_test, 'data/predictions/Random_Forest_Regression.csv')\n",
    "\n",
    "# XGBoost Regressor\n",
    "model_xgb = XGBRegressor(objective='reg:squarederror', n_estimators=10000, learning_rate=0.1, n_jobs=-1)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "save_predictions_to_csv(model_xgb, X_test, y_test, 'data/predictions/XGBoost_Regression.csv')\n",
    "\n",
    "# Logistic Regression\n",
    "model_lr = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "model_lr.fit(X_train, y_train)\n",
    "save_predictions_to_csv(model_lr, X_test, y_test, 'data/predictions/Logistic_Regression.csv')\n",
    "\n",
    "# CatBoost Regressor\n",
    "model_cb = CatBoostRegressor(iterations=100000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)\n",
    "model_cb.fit(X_train, y_train)\n",
    "save_predictions_to_csv(model_cb, X_test, y_test, 'data/predictions/CatBoost_Regression.csv')\n",
    "\n",
    "# Extra Trees Regressor\n",
    "model_et = ExtraTreesRegressor(n_estimators=10000, n_jobs=-1)\n",
    "model_et.fit(X_train, y_train)\n",
    "save_predictions_to_csv(model_et, X_test, y_test, 'data/predictions/Extra_Trees_Regression.csv')"
   ],
   "id": "361c2f5fed98ef3c",
   "outputs": [],
   "execution_count": 88
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
