{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:05:05.140047Z",
     "start_time": "2025-01-20T00:05:05.122047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, make_scorer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge, LogisticRegression, HuberRegressor, TheilSenRegressor, PoissonRegressor, TweedieRegressor, GammaRegressor, SGDRegressor, OrthogonalMatchingPursuit, PassiveAggressiveRegressor, RANSACRegressor, ElasticNetCV, OrthogonalMatchingPursuitCV, LarsCV, LassoCV, RidgeCV, ARDRegression, LassoLars\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor,HistGradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, VotingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "id": "e8c056c1d9f28e8d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:05:06.654143Z",
     "start_time": "2025-01-20T00:05:06.611144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load CSV data into a pandas DataFrame\n",
    "df = pd.read_csv('data/Battery_RUL_cleaned.csv')\n",
    "#df = pd.read_csv('data/Battery_RUL.csv')\n",
    "df = df[df.columns[1:]]  # Remove the first column\n",
    "\n",
    "# Last column is the target variable\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "d1aeaf59b6a1f90e",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T23:39:20.277277Z",
     "start_time": "2025-01-19T23:39:20.270276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_grid_search_results(results, param1, param2, score1, score2):\n",
    "    \"\"\"\n",
    "    Plots the grid search results with two y-axes for different hyperparameter values.\n",
    "\n",
    "    Parameters:\n",
    "    - results: DataFrame containing the GridSearchCV results.\n",
    "    - param1: The first hyperparameter to plot on the x-axis.\n",
    "    - param2: The second hyperparameter to differentiate the lines.\n",
    "    - score1: The first score to plot on the primary y-axis.\n",
    "    - score2: The second score to plot on the secondary y-axis.\n",
    "    \"\"\"\n",
    "    # Create a single plot with two y-axes\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot the first score on the primary y-axis\n",
    "    for value in results[param2].unique():\n",
    "        subset = results[results[param2] == value]\n",
    "        ax1.plot(subset[param1], -subset[score1], label=f'{score1} - {param2}: {value}', linestyle='-', marker='o')\n",
    "    ax1.set_xlabel(param1)\n",
    "    ax1.set_ylabel(score1)\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.set_title('Grid Search Results')\n",
    "\n",
    "    # Create a secondary y-axis for the second score\n",
    "    ax2 = ax1.twinx()\n",
    "    for value in results[param2].unique():\n",
    "        subset = results[results[param2] == value]\n",
    "        ax2.plot(subset[param1], -subset[score2], label=f'{score2} - {param2}: {value}', linestyle='--', marker='x')\n",
    "    ax2.set_ylabel(score2)\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "4e765e1fc194dd80",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:31:30.786932Z",
     "start_time": "2025-01-20T00:30:01.634713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define models to evaluate\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(n_jobs=-1),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"Support Vector Regressor\": SVR(),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=1000, n_jobs=-1),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=1000),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsRegressor(n_jobs=-1),\n",
    "    \"Multi-layer Perceptron\": MLPRegressor(),\n",
    "    \"Elasitc Net\": ElasticNet(),\n",
    "    \"Bayesian Ridge\": BayesianRidge(),\n",
    "    \"Extra Trees\": ExtraTreesRegressor(n_estimators=1000, n_jobs=-1),\n",
    "    \"AdaBoost\": AdaBoostRegressor(),\n",
    "    \"Bagging\": BaggingRegressor(n_estimators=1000, n_jobs=-1),\n",
    "    \"Hist Gradient Boosting\": HistGradientBoostingRegressor(),\n",
    "    \"CatBoost\": CatBoostRegressor(n_estimators=1000, verbose=0),\n",
    "    \"LightGBM\": LGBMRegressor(n_estimators=1000, n_jobs=-1),\n",
    "    \"XGBoost\": XGBRegressor(objective='reg:squarederror', n_estimators=1000, n_jobs=-1),\n",
    "    #\"Logistic Regression\": LogisticRegression(max_iter=1000, n_jobs=-1),\n",
    "    \"Gaussian Process\": GaussianProcessRegressor(),\n",
    "    \"Huber Regressor\": HuberRegressor(),\n",
    "    \"Theil-Sen Regressor\": TheilSenRegressor(),\n",
    "    \"Poisson Regressor\": PoissonRegressor(),\n",
    "    \"Tweedie Regressor\": TweedieRegressor(),\n",
    "    \"Gamma Regressor\": GammaRegressor(),\n",
    "    #\"SGD Regressor\": SGDRegressor(),\n",
    "    \"Orthogonal Matching Pursuit\": OrthogonalMatchingPursuit(),\n",
    "    \"Passive Aggressive Regressor\": PassiveAggressiveRegressor(),\n",
    "    \"RANSAC Regressor\": RANSACRegressor(),\n",
    "    \"Elastic Net CV\": ElasticNetCV(),\n",
    "    \"Orthogonal Matching Pursuit CV\": OrthogonalMatchingPursuitCV(),\n",
    "    \"Lars CV\": LarsCV(),\n",
    "    \"Lasso CV\": LassoCV(),\n",
    "    \"Ridge CV\": RidgeCV(),\n",
    "    \"ARD Regression\": ARDRegression(),\n",
    "    \"Lasso Lars\": LassoLars(),\n",
    "    \"Isotonic Regression\": IsotonicRegression()\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize a dataframe to store results\n",
    "results = []\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    if name == \"Isotonic Regression\":\n",
    "        X_train_model = X_train[:, 0].reshape(-1, 1)  # Select a single feature\n",
    "        X_test_model = X_test[:, 0].reshape(-1, 1)\n",
    "    else:\n",
    "        X_train_model = X_train\n",
    "        X_test_model = X_test\n",
    "    model.fit(X_train_model, y_train)\n",
    "    y_pred = model.predict(X_test_model)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results.append({\"Model\": name,\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2})\n",
    "    \n",
    "    \n",
    "\n",
    "# Convert results to a DataFrame and rank them\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"MAE\", ascending=True)\n",
    "results_df.to_csv('data/ranking.csv', index=False)\n",
    "print(results_df)\n"
   ],
   "id": "2fe90323d8a9aaa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.183e+07, tolerance: 1.015e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\modes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.210e+07, tolerance: 1.015e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\modes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1769\n",
      "[LightGBM] [Info] Number of data points in the train set: 10799, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 583.079452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\modes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\modes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:285: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "C:\\Users\\modes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:285: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Model         MAE            MSE        RMSE  \\\n",
      "11                     Extra Trees    6.405731     162.577321   12.750581   \n",
      "13                         Bagging    7.931399     232.256216   15.239955   \n",
      "4                    Random Forest    7.937425     232.661285   15.253238   \n",
      "17                         XGBoost    8.668144     245.356720   15.663867   \n",
      "6                    Decision Tree    8.943704     556.126852   23.582342   \n",
      "16                        LightGBM    8.973900     239.627970   15.479922   \n",
      "15                        CatBoost   14.020744     410.249081   20.254606   \n",
      "5                Gradient Boosting   15.482934     505.434515   22.481871   \n",
      "14          Hist Gradient Boosting   16.404646     563.925931   23.747125   \n",
      "7              K-Nearest Neighbors   22.810074    1374.014252   37.067698   \n",
      "1                 Ridge Regression   34.076042    2103.815289   45.867366   \n",
      "31                        Ridge CV   34.124713    2081.687420   45.625513   \n",
      "32                  ARD Regression   34.138231    2080.600883   45.613604   \n",
      "29                         Lars CV   34.142392    2080.856114   45.616402   \n",
      "0                Linear Regression   34.142392    2080.856114   45.616402   \n",
      "26                RANSAC Regressor   34.173863    2075.549565   45.558200   \n",
      "34             Isotonic Regression   35.439309    2248.574201   47.419133   \n",
      "10                  Bayesian Ridge   35.803766    2331.903393   48.289786   \n",
      "28  Orthogonal Matching Pursuit CV   35.804113    2331.646044   48.287121   \n",
      "33                      Lasso Lars   35.805559    2331.591595   48.286557   \n",
      "9                      Elasitc Net   35.818380    2339.068964   48.363922   \n",
      "2                 Lasso Regression   35.818956    2339.026153   48.363480   \n",
      "22               Tweedie Regressor   36.023266    2365.575840   48.637186   \n",
      "27                  Elastic Net CV   36.626151    2424.090338   49.235052   \n",
      "30                        Lasso CV   36.654860    2427.652696   49.271216   \n",
      "24     Orthogonal Matching Pursuit   36.795948    2430.978535   49.304954   \n",
      "20             Theil-Sen Regressor   36.798785    2356.578713   48.544605   \n",
      "8           Multi-layer Perceptron   37.576770    2566.849726   50.664087   \n",
      "19                 Huber Regressor   37.897882    2644.002679   51.419867   \n",
      "12                        AdaBoost   38.911377    2316.770473   48.132842   \n",
      "25    Passive Aggressive Regressor   39.171970    2803.675244   52.949743   \n",
      "3         Support Vector Regressor   39.981383    3008.742449   54.852005   \n",
      "23                 Gamma Regressor   73.934204  263931.987250  513.743114   \n",
      "21               Poisson Regressor  263.032811   92577.438969  304.265409   \n",
      "18                Gaussian Process  567.522317  419274.500813  647.514093   \n",
      "\n",
      "          R2  \n",
      "11  0.998244  \n",
      "13  0.997491  \n",
      "4   0.997486  \n",
      "17  0.997349  \n",
      "6   0.993992  \n",
      "16  0.997411  \n",
      "15  0.995568  \n",
      "5   0.994539  \n",
      "14  0.993908  \n",
      "7   0.985156  \n",
      "1   0.977271  \n",
      "31  0.977510  \n",
      "32  0.977522  \n",
      "29  0.977519  \n",
      "0   0.977519  \n",
      "26  0.977577  \n",
      "34  0.975707  \n",
      "10  0.974807  \n",
      "28  0.974810  \n",
      "33  0.974810  \n",
      "9   0.974730  \n",
      "2   0.974730  \n",
      "22  0.974443  \n",
      "27  0.973811  \n",
      "30  0.973773  \n",
      "24  0.973737  \n",
      "20  0.974540  \n",
      "8   0.972269  \n",
      "19  0.971435  \n",
      "12  0.974970  \n",
      "25  0.969710  \n",
      "3   0.967495  \n",
      "23 -1.851427  \n",
      "21 -0.000173  \n",
      "18 -3.529691  \n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:05:40.719744Z",
     "start_time": "2025-01-20T00:05:40.684744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#model = LGBMRegressor(n_estimators=10000, n_jobs=-1)\n",
    "#model = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "#Mean Absolute Error: 7.9007\n",
    "#Mean Squared Error: 211.5364\n",
    "#model = GradientBoostingRegressor(n_estimators=1000, max_depth=6)\n",
    "#Mean Absolute Error: 9.1008\n",
    "#Mean Squared Error: 256.1412\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ],
   "id": "f029fa143187a462",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 35.4393\n",
      "Mean Squared Error: 2248.5742\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T23:39:26.356680Z",
     "start_time": "2025-01-18T23:38:53.440616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the hyperparameter grid\n",
    "#param_grid = {\n",
    "#    \"n_estimators\": [100, 200, 500],\n",
    "#    \"max_depth\": [10, 20, 50, None],\n",
    "#    \"min_samples_split\": [2, 5, 10],\n",
    "#    \"min_samples_leaf\": [1, 2, 4],\n",
    "#    \"max_features\": [None, \"sqrt\", \"log2\"]}\n",
    "\n",
    "# Define the scoring dictionary\n",
    "#scoring = {\n",
    "#    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "#    'MSE': make_scorer(mean_squared_error, greater_is_better=False)}\n",
    "\n",
    "# Initialize the GridSearchCV with n_jobs=1\n",
    "model = ExtraTreesRegressor(n_estimators=10000, n_jobs=-1)\n",
    "#model = GridSearchCV(ExtraTreesRegressor(), param_grid, cv=5, scoring=scoring, refit='MSE', n_jobs=1, verbose=3)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Extract the results and convert to a DataFrame\n",
    "#results = pd.DataFrame(model.cv_results_)\n",
    "#results.to_csv('data/ETR1.csv', index=False)\n",
    "\n",
    "# Get the best model\n",
    "#best_model = model.best_estimator_\n",
    "#print(best_model)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ],
   "id": "1234811bb559d5b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 6.3922\n",
      "Mean Squared Error: 161.5680\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the hyperparameter grid\n",
    "#param_grid = [\n",
    "#    {'C': [1e+4, 1e+5], 'kernel': ['rbf'], 'gamma': [1e-5, 1e-4]}]\n",
    "# Define the scoring dictionary\n",
    "#scoring = {\n",
    "#    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "#    'MSE': make_scorer(mean_squared_error, greater_is_better=False)}\n",
    "# Initialize and train the SVM model\n",
    "model = SVR(kernel='linear', C=0.0001)\n",
    "# Mean Absolute Error: 35.6885\n",
    "# Mean Squared Error: 2358.7762\n",
    "#model = GridSearchCV(SVR(), param_grid, cv=5, scoring=scoring, refit='MSE', n_jobs=-1, verbose=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Extract the results and convert to a DataFrame\n",
    "#results = pd.DataFrame(model.cv_results_)\n",
    "#results.to_csv('data/SVR2.csv', index=False)\n",
    "\n",
    "#plot_grid_search_results(results, 'param_C', 'param_gamma', 'mean_test_MSE', 'mean_test_MAE')\n",
    "\n",
    "# Get the best model\n",
    "#best_model = model.best_estimator_\n",
    "#print(best_model)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "# first run: 39.9814"
   ],
   "id": "2092498e7cfdfb6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:29:19.381089Z",
     "start_time": "2025-01-18T00:29:12.112078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the hyperparameter grid\n",
    "#param_grid = [\n",
    "#    {'max_depth': [None, 1, 2, 3, 4, 5, 6], 'n_estimators': [1000] }]\n",
    "# Define the scoring dictionary\n",
    "#scoring = {\n",
    "#    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "#    'MSE': make_scorer(mean_squared_error, greater_is_better=False)}\n",
    "# Initialize and train the Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=1000,  n_jobs=-1)\n",
    "#Mean Absolute Error: 7.9780\n",
    "#Mean Squared Error: 236.9407\n",
    "#model = GridSearchCV(RandomForestRegressor(), param_grid, cv=5, scoring=scoring, refit='MSE', n_jobs=-1, verbose=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Extract the results and convert to a DataFrame\n",
    "#results = pd.DataFrame(model.cv_results_)\n",
    "#results.to_csv('data/RFR1.csv', index=False)\n",
    "\n",
    "# Plot the grid search results\n",
    "#plot_grid_search_results(results, 'param_max_depth', 'param_n_estimators', 'mean_test_MSE', 'mean_test_MAE')\n",
    "\n",
    "# Get the best model\n",
    "#best_model = model.best_estimator_\n",
    "#print(best_model)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "# first run: 8.0703\n",
    "# best run: 7.9780"
   ],
   "id": "7ac2e03657bdb3e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 7.9351\n",
      "Mean Squared Error: 233.6358\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:17:39.457307Z",
     "start_time": "2025-01-18T00:17:31.612297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the hyperparameter grid\n",
    "#param_grid = [\n",
    "#    {'n_estimators': [1000, 10000, 100000, 1000000], 'learning_rate':[0.1], 'objective': ['reg:squarederror']}]\n",
    "# Define the scoring dictionary\n",
    "#scoring = {\n",
    "#    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "#    'MSE': make_scorer(mean_squared_error, greater_is_better=False)}\n",
    "# Initialize and train the XGBoost model\n",
    "model = XGBRegressor(objective='reg:squarederror', n_estimators=10000, learning_rate=0.1, n_jobs=-1)\n",
    "#Mean Absolute Error: 7.9632\n",
    "#Mean Squared Error: 210.3707\n",
    "#model = GridSearchCV(xgb.XGBRegressor(), param_grid, cv=5, scoring=scoring, refit='MSE', n_jobs=-1, verbose=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Extract the results and convert to a DataFrame\n",
    "#results = pd.DataFrame(model.cv_results_)\n",
    "#results.to_csv('data/XGB2.csv', index=False)\n",
    "\n",
    "# Plot the grid search results\n",
    "#plot_grid_search_results(results, 'param_n_estimators', 'param_learning_rate','mean_test_MSE', 'mean_test_MAE')\n",
    "\n",
    "# Get the best model\n",
    "#best_model = model.best_estimator_\n",
    "#print(best_model)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "# first run: 15.3975\n",
    "# best run: 7.9632"
   ],
   "id": "60f9f9bd5531afce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 7.9632\n",
      "Mean Squared Error: 210.3707\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T14:26:15.987991Z",
     "start_time": "2025-01-17T14:22:59.527698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# first run: 39.9814"
   ],
   "id": "4d3238d066f679f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 47.2215\n",
      "Mean Squared Error: 3565.5578\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:44:07.692138Z",
     "start_time": "2025-01-18T00:39:55.658762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train the CatBoost Regressor\n",
    "model = CatBoostRegressor(iterations=100000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "# best run:\n",
    "# Mean Absolute Error: 7.5714\n",
    "# Mean Squared Error: 186.3288"
   ],
   "id": "3873120eb6ba53b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 7.5714\n",
      "Mean Squared Error: 186.3288\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:52:43.193449Z",
     "start_time": "2025-01-18T00:52:25.794424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train the Gaussian Process Regressor\n",
    "model = GaussianProcessRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n"
   ],
   "id": "50bc2f0dc3bbd2b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 567.5223\n",
      "Mean Squared Error: 419274.5008\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T00:23:31.934392Z",
     "start_time": "2025-01-19T00:13:55.746184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to save real and predicted values to a CSV file\n",
    "def save_predictions_to_csv(model, X_test, y_test, filename):\n",
    "    y_pred = model.predict(X_test)\n",
    "    results_df = pd.DataFrame({'Real': y_test, 'Predicted': y_pred})\n",
    "    results_df.to_csv(filename, index=False)\n",
    "\n",
    "# Support Vector Regressor\n",
    "model_svr = SVR(kernel='linear', C=0.0001)\n",
    "model_svr.fit(X_train, y_train)\n",
    "save_predictions_to_csv(model_svr, X_test, y_test, 'data/predictions/Support_Vector_Regression.csv')\n",
    "\n",
    "# Random Forest Regressor\n",
    "model_rf = RandomForestRegressor(n_estimators=1000, n_jobs=-1)\n",
    "model_rf.fit(X_train, y_train)\n",
    "save_predictions_to_csv(model_rf, X_test, y_test, 'data/predictions/Random_Forest_Regression.csv')\n",
    "\n",
    "# XGBoost Regressor\n",
    "model_xgb = XGBRegressor(objective='reg:squarederror', n_estimators=10000, learning_rate=0.1, n_jobs=-1)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "save_predictions_to_csv(model_xgb, X_test, y_test, 'data/predictions/XGBoost_Regression.csv')\n",
    "\n",
    "# Logistic Regression\n",
    "model_lr = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "model_lr.fit(X_train, y_train)\n",
    "save_predictions_to_csv(model_lr, X_test, y_test, 'data/predictions/Logistic_Regression.csv')\n",
    "\n",
    "# CatBoost Regressor\n",
    "model_cb = CatBoostRegressor(iterations=100000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)\n",
    "model_cb.fit(X_train, y_train)\n",
    "save_predictions_to_csv(model_cb, X_test, y_test, 'data/predictions/CatBoost_Regression.csv')\n",
    "\n",
    "# Extra Trees Regressor\n",
    "model_et = ExtraTreesRegressor(n_estimators=10000, n_jobs=-1)\n",
    "model_et.fit(X_train, y_train)\n",
    "save_predictions_to_csv(model_et, X_test, y_test, 'data/predictions/Extra_Trees_Regression.csv')"
   ],
   "id": "361c2f5fed98ef3c",
   "outputs": [],
   "execution_count": 88
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
